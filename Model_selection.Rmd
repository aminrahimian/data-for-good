---
title: "Model Selection Tutorial"
author: "Annie Goodwin and Erin O'Neill"
date: "2022-11-15"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Model Selection and Regulation:

## Background reading:
### Intro to Statistical Learning: 
6.1 and 6.2

### Ethical Algorithm:
Chapter 2

## Dataset:
Dataset on Kaggle: https://www.kaggle.com/datasets/saravananselvamohan/freddie-mac-singlefamily-loanlevel-dataset

Freddie Mac Single Family Loan-Level Dataset: https://www.freddiemac.com/research/datasets/sf-loanlevel-dataset

Description of data fields: https://www.freddiemac.com/fmac-resources/research/pdf/file_layout.xlsx

The Federal Home Loan Mortgage Corporation, commonly known as Freddie Mac, is a publicly traded, government-sponsored enterprise, headquartered in Tysons Corner, Virginia. The FHLMC was created in 1970 to expand the secondary market for mortgages in the US: https://en.wikipedia.org/wiki/Freddie_Mac

The Federal National Mortgage Association, commonly known as Fannie Mae, is a United States government-sponsored enterprise and, since 1968, a publicly traded company: https://en.wikipedia.org/wiki/Fannie_Mae

The primary difference between Freddie Mac and Fannie Mae is where they source their mortgages from. Fannie Mae buys mortgages from larger, commercial banks, while Freddie Mac buys them from much smaller banks.

Dataset: Home Mortgage Disclosure Act, National Loan Applications for 2020 https://ffiec.cfpb.gov/data-publication/dynamic-national-loan-level-dataset/2020

Data field definitions/values: https://ffiec.cfpb.gov/documentation/2020/lar-data-fields/

In this tutorial we will use the dataset with the following attributes as an example: 31 Attributes In the dataset. We are going to use the following:

Credit Score
First Time Homebuyer flag
Original Debt to Income Ratio
Number of Borrowers
Original Interest Rate - this is what we want to predict
This is a case of prediction because we want to predict a buyer’s mortgage interest rate (outcome) based on application information (predictors)

# Essence of Data:

## Why might you want to use model selection:
It can be easy to over fit a model if not using model selection. Using model Selection can help you create a model that best balances the bias variance trade off.

## HOw to compare different models?
There are many different model selection criterion that can be used to compare models. AIC, BIC, Mallow's CP, adjusted $R^2$ and other selection criterion that penalize more complex models used to be the best way to compare models. However, with increase of computing process, validation and cross validation have become a more accurate and still accessible way of comparing models. With new computational power, we can now calculate MSE of a test set as well as use cross validation and MSE to work with parameter tuning in certain models.


## Forward vs backwards selection
### Forward selection:
Begins with a model that contains no variables (called the Null Model)
Begin adding the most significant variables one after the other
Variable significance can be calculated using a statistical significance test such as an f-test or t-test. The most significant variable will have the lowest p-value.
Continue to add significant variables until a pre-specified stopping rule is reached or until all the variables under consideration are included in the model
The pre-specified stopping rule could include a specific test of model prediction accuracy or a certain error level (such as MSE or MAE)
### Backwards selection:
Begins with a model that contains all variables under consideration (called the Full Model)
Then starts removing the least significant variables one after the other.
Variable significance can be calculated using a statistical significance test such as an f-test or t-test. The least significant variable will have the highest p-value.
Until a pre-specified stopping rule is reached or until no variable is left in the model.



# Lasso and Ridge Regression:

## Ridge Regression:
Ridge regression is similar to linear regression however it adds a penalty term ($\lambda*slope^2$). This penalty introduces a small amount of bias on how the regression line fits the data and in turn lessens the variance. Ridge regression minimizes $SSR + \lambda *slope^2$. The severity of the penalty is determined by the non-negative value of $lambda$.You can use cross validation to select the best value of $lambda$ for your model. The larger we make $lambda$, the slope will get closer and closer to zero, making the prediction less and less sensitive to our predictor variables. 

Ridge regression can also be applied with categorical predictors and/or with logistic regression. The same penalty term and general concepts apply with these other forms of ridge regression.

With least squares, you need at least p data points to estimate a model with p parameters, however, one benefit of ridge regression is that the penalty term allows us to find a solution with cross validation when the sample size is less than the penalty.

## Lasso Regression:
LASSO stands for Least Absolute Shrinkage and Selection Operator. This regression analysis method is useful for prediction accuracy and interpretability of statistical models. The model uses shrinkage which is where data values are shrunk towards a central point as the mean, thus encouraging simple and sparse models.

Lasso regression is very similar to ridge regression, but instead of being penalized by $\lambda * slope^2$, the model is penalized by $|\lambda|$. Lasso regression also increases bias in hopes of decreasing variance. Ridge and Lasso regression might shrink different parameters by different amounts. Ridge regression can only shrink the slope of the model close to zero, while Lasso can shrink the parameters all the way to zero. This means that Lasso regression can remove useless parameters.

Ridge regression tends to do a little better when all of the variables are useful, while Lasso does well with models that have some useless variables.

Now, let's run some code:
## Import Packages
```{r}
library(readr)
library(tidyr)
library(glmnet)
library(leaps)
```


```{r pressure, echo=FALSE}
loan_level_500k <- read.csv("loan_level_500k.csv")
```

```{r}
cols <- c('ORIGINAL_INTEREST_RATE', 'CREDIT_SCORE', 'FIRST_TIME_HOMEBUYER_FLAG','ORIGINAL_DEBT_TO_INCOME_RATIO', 'NUMBER_OF_BORROWERS','POSTAL_CODE','LOAN_PURPOSE')
selected_df <- loan_level_500k[cols]
selected_df <- selected_df %>% drop_na()
loan_level_500k <- selected_df

```

```{r}
#Create a training and test set with an 70/30 split.
set.seed(8)
dt <- sort(sample(nrow(loan_level_500k), nrow(loan_level_500k)*.7))
train <- loan_level_500k[dt,]
test <- loan_level_500k[-dt,]
```

#Run Linear Regression With all predictors
```{r}
lm.fit <- lm(train$ORIGINAL_INTEREST_RATE~., data = train)
predict <- predict.lm(lm.fit, newdata= test)
```

Now we will try forward selection to find the best multiple linear regression model.
```{r, warning = FALSE}
#Fit Models with forward selection
regfit.fwd <- regsubsets(train$ORIGINAL_INTEREST_RATE~ ., data = train, nvmax=7, method ="forward")
summary(regfit.fwd)

#Backwards Selection 
regfit.bwd <- regsubsets (train$ORIGINAL_INTEREST_RATE~ ., data = train, nvmax=7, method = "backward")
summary(regfit.bwd)

test.mat <- model.matrix(test$ORIGINAL_INTEREST_RATE ~., data = test)
regfit.fwd

#Calculate MSE for each model size
val.errors <- rep (NA , 7)
for (i in 1:7) {
  coefi <- coef ( regfit.fwd , id = i)
  pred <- test.mat[, names ( coefi )] %*% coefi
  val.errors [i] <- mean (( test$ORIGINAL_INTEREST_RATE -pred ) ^2)
}
val.errors

#Find the model with the smallest MSE

MSE.Forward <- val.errors[which.min(val.errors)]
MSE.Forward
which.min(val.errors)
```


Next we will run Ridge Regression
```{r}
#Format data and grid to prep for Ridge Regression
x <- model.matrix(ORIGINAL_INTEREST_RATE~., loan_level_500k)[,-1]
y <- loan_level_500k$ORIGINAL_INTEREST_RATE
y.test <-y[-dt]
grid <- 10^seq(10, -2, length = 100)

#Fit the Model
ridge.mod <- glmnet(x[-dt,], y[-dt], alpha = 0, lamda = grid, thresh = 1e-12)

set.seed(30)

#Use Cross validation to find best value of lambda
cv.out <- cv.glmnet(x[dt,], y[dt], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam

#Fit predictions with best value of lambda
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[-dt, ])

#Calculate MSE
MSE.ridge <- mean((ridge.pred - y.test)^2)
MSE.ridge

```
The best value for Lambda is .0114114 and yields an MSE of .3124863.

```{r}
#LASSO Regression
set.seed(80)
lasso.mod <- glmnet(x[-dt, ], y[-dt], alpha=1, lambda = grid)

plot(lasso.mod)

set.seed(1)
cv.out <- cv.glmnet(x[dt, ], y[dt], alpha=1)
plot(cv.out)
bestlam <-cv.out$lambda.min
bestlam
lasso.pred <- predict(lasso.mod, s = bestlam, newx =x[-dt, ])

MSE.LASSO <- mean((lasso.pred -y.test)^2)
MSE.LASSO
```
The best lambda value of .0002698215 returns and MSE of .3131287.
 
The MSE of our models are all rather similar, however you can see that Ridge regression out performs Lasso which out performs forward selection.


# Ethics
## What are protected characteristics?
https://www.youtube.com/watch?v=VXLtKlmtrvM&t=170s 

## Are they really protected:
Maybe? Many people are trying to protect them. It can hard to prove discrimination. Humans are biased and therefore the models and programs they make and the data used to train models are biased.


### Examples in Data Science
1. In October 2019, researchers found that an algorithm used on more than 200 million people in US hospitals to predict which patients would likely need extra medical care heavily favored white patients over black patients. While race itself wasn’t a variable used in this algorithm, another variable highly correlated to race was, which was healthcare cost history. The rationale was that cost summarizes how many healthcare needs a particular person has. For various reasons, black patients incurred lower health-care costs than white patients with the same conditions on average.

2. Thankfully, researchers worked with Optum to reduce the level of bias by 80%. But had they not interrogated in the first place, AI bias would have continued to discriminate severely.

Arguably the most notable example of AI bias is the COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) algorithm used in US court systems to predict the likelihood that a defendant would become a recidivist.

Due to the data that was used, the model that was chosen, and the process of creating the algorithm overall, the model predicted twice as many false positives for recidivism for black offenders (45%) than white offenders (23%).

3. Amazon’s one of the largest tech giants in the world. And so, it’s no surprise that they’re heavy users of machine learning and artificial intelligence. In 2015, Amazon realized that their algorithm used for hiring employees was found to be biased against women. The reason for that was because the algorithm was based on the number of resumes submitted over the past ten years, and since most of the applicants were men, it was trained to favor men over women.


### Ways Forward:
1. Try to have models reflect how decisions should be made, not how they have been made historically. We don't want to perpetuate past biases.

2. Check prediction accuracy (false negatives, false positive rate, etc.) across social groups to catch models that might be discriminatory.

3. Consider ways of enforcing data governance and fairness. 

Source/further reading: https://towardsdatascience.com/real-life-examples-of-discriminating-artificial-intelligence-cae395a90070 
