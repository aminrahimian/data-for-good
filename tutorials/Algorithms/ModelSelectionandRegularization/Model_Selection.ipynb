{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80133827",
   "metadata": {},
   "source": [
    "### Intro to Statistical Learning: \n",
    "6.1 and 6.2\n",
    "\n",
    "### Ethical Algorithm:\n",
    "Chapter 2\n",
    "\n",
    "## Dataset:\n",
    "Dataset on Kaggle: https://www.kaggle.com/datasets/saravananselvamohan/freddie-mac-singlefamily-loanlevel-dataset\n",
    "\n",
    "Freddie Mac Single Family Loan-Level Dataset: https://www.freddiemac.com/research/datasets/sf-loanlevel-dataset\n",
    "\n",
    "Description of data fields: https://www.freddiemac.com/fmac-resources/research/pdf/file_layout.xlsx\n",
    "\n",
    "The Federal Home Loan Mortgage Corporation, commonly known as Freddie Mac, is a publicly traded, government-sponsored enterprise, headquartered in Tysons Corner, Virginia. The FHLMC was created in 1970 to expand the secondary market for mortgages in the US: https://en.wikipedia.org/wiki/Freddie_Mac\n",
    "\n",
    "The Federal National Mortgage Association, commonly known as Fannie Mae, is a United States government-sponsored enterprise and, since 1968, a publicly traded company: https://en.wikipedia.org/wiki/Fannie_Mae\n",
    "\n",
    "The primary difference between Freddie Mac and Fannie Mae is where they source their mortgages from. Fannie Mae buys mortgages from larger, commercial banks, while Freddie Mac buys them from much smaller banks.\n",
    "\n",
    "Dataset: Home Mortgage Disclosure Act, National Loan Applications for 2020 https://ffiec.cfpb.gov/data-publication/dynamic-national-loan-level-dataset/2020\n",
    "\n",
    "Data field definitions/values: https://ffiec.cfpb.gov/documentation/2020/lar-data-fields/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78cb9ad",
   "metadata": {},
   "source": [
    "### In this tutorial we will use the dataset with the following attributes as an example: 31 Attributes In the dataset. We are going to use the following:\n",
    "\n",
    "Credit Score\n",
    "First Time Homebuyer flag\n",
    "Original Debt to Income Ratio\n",
    "Number of Borrowers\n",
    "Original Interest Rate - this is what we want to predict\n",
    "This is a case of prediction because we want to predict a buyerâ€™s mortgage interest rate (outcome) based on application information (predictors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a01a6c",
   "metadata": {},
   "source": [
    "\n",
    "# Essence of Data:\n",
    "\n",
    "## Why might you want to use model selection:\n",
    "It can be easy to over fit a model if not using model selection. Using model Selection can help you create a model that best balances the bias variance trade off.\n",
    "\n",
    "## How to compare different models?\n",
    "There are many different model selection criterion that can be used to compare models. AIC, BIC, Mallow's CP, adjusted $R^2$ and other selection criterion that penalize more complex models used to be the best way to compare models. However, with increase of computing process, validation and cross validation have become a more accurate and still accessible way of comparing models. With new computational power, we can now calculate MSE of a test set as well as use cross validation and MSE to work with parameter tuning in certain models.\n",
    "\n",
    "\n",
    "## Forward vs backwards selection\n",
    "### Forward selection:\n",
    "Begins with a model that contains no variables (called the Null Model)\n",
    "Begin adding the most significant variables one after the other\n",
    "Variable significance can be calculated using a statistical significance test such as an f-test or t-test. The most significant variable will have the lowest p-value.\n",
    "Continue to add significant variables until a pre-specified stopping rule is reached or until all the variables under consideration are included in the model\n",
    "The pre-specified stopping rule could include a specific test of model prediction accuracy or a certain error level (such as MSE or MAE)\n",
    "\n",
    "\n",
    "### Backwards selection:\n",
    "Begins with a model that contains all variables under consideration (called the Full Model)\n",
    "Then starts removing the least significant variables one after the other.\n",
    "Variable significance can be calculated using a statistical significance test such as an f-test or t-test. The least significant variable will have the highest p-value.\n",
    "Until a pre-specified stopping rule is reached or until no variable is left in the model.\n",
    "\n",
    "\n",
    "\n",
    "# Lasso and Ridge Regression:\n",
    "\n",
    "## Ridge Regression:\n",
    "Ridge regression is similar to linear regression however it adds a penalty term ($\\lambda*slope^2$). This penalty introduces a small amount of bias on how the regression line fits the data and in turn lessens the variance. Ridge regression minimizes $SSR + \\lambda *slope^2$. The severity of the penalty is determined by the non-negative value of $lambda$.You can use cross validation to select the best value of $lambda$ for your model. The larger we make $lambda$, the slope will get closer and closer to zero, making the prediction less and less sensitive to our predictor variables. \n",
    "\n",
    "Ridge regression can also be applied with categorical predictors and/or with logistic regression. The same penalty term and general concepts apply with these other forms of ridge regression.\n",
    "\n",
    "With least squares, you need at least p data points to estimate a model with p parameters, however, one benefit of ridge regression is that the penalty term allows us to find a solution with cross validation when the sample size is less than the penalty.\n",
    "\n",
    "## Lasso Regression:\n",
    "LASSO stands for Least Absolute Shrinkage and Selection Operator. This regression analysis method is useful for prediction accuracy and interpretability of statistical models. The model uses shrinkage which is where data values are shrunk towards a central point as the mean, thus encouraging simple and sparse models.\n",
    "\n",
    "Lasso regression is very similar to ridge regression, but instead of being penalized by $\\lambda * slope^2$, the model is penalized by $|\\lambda|$. Lasso regression also increases bias in hopes of decreasing variance. Ridge and Lasso regression might shrink different parameters by different amounts. Ridge regression can only shrink the slope of the model close to zero, while Lasso can shrink the parameters all the way to zero. This means that Lasso regression can remove useless parameters.\n",
    "\n",
    "Ridge regression tends to do a little better when all of the variables are useful, while Lasso does well with models that have some useless variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7c06da",
   "metadata": {},
   "source": [
    "# Now, let's run some code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "94596a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d82dc706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "df = pd.read_csv('loan_level_500k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84b279ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the features we need\n",
    "cols = ['ORIGINAL_INTEREST_RATE', 'CREDIT_SCORE', 'FIRST_TIME_HOMEBUYER_FLAG',\n",
    "        'ORIGINAL_DEBT_TO_INCOME_RATIO', 'NUMBER_OF_BORROWERS', 'POSTAL_CODE', 'LOAN_PURPOSE']\n",
    "df_selected = df[cols]\n",
    "# drop null values\n",
    "df_selected = df_selected.dropna()\n",
    "\n",
    "# Convert categorical variables to dummy variables\n",
    "df_selected = pd.get_dummies(df_selected, drop_first=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "694f9b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training and test set with an 70/30 split.\n",
    "X = df_selected.drop('ORIGINAL_INTEREST_RATE', axis=1)\n",
    "y = df_selected['ORIGINAL_INTEREST_RATE']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "# Convert bool to int\n",
    "X_train = X_train.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0a7fff60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE: 0.33249260231929145\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression with All Predictors\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "y_pred = lm.predict(X_test)\n",
    "mse_linear = mean_squared_error(y_test, y_pred)\n",
    "print(f'Linear Regression MSE: {mse_linear}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2e00436a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              OLS Regression Results                              \n",
      "==================================================================================\n",
      "Dep. Variable:     ORIGINAL_INTEREST_RATE   R-squared:                       0.059\n",
      "Model:                                OLS   Adj. R-squared:                  0.059\n",
      "Method:                     Least Squares   F-statistic:                     2261.\n",
      "Date:                    Thu, 03 Oct 2024   Prob (F-statistic):               0.00\n",
      "Time:                            16:51:01   Log-Likelihood:            -2.2075e+05\n",
      "No. Observations:                  253292   AIC:                         4.415e+05\n",
      "Df Residuals:                      253284   BIC:                         4.416e+05\n",
      "Df Model:                               7                                         \n",
      "Covariance Type:                nonrobust                                         \n",
      "=================================================================================================\n",
      "                                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------\n",
      "const                             8.1744      0.017    468.883      0.000       8.140       8.209\n",
      "CREDIT_SCORE                     -0.0015    2.2e-05    -66.862      0.000      -0.002      -0.001\n",
      "ORIGINAL_DEBT_TO_INCOME_RATIO     0.0032      0.000     30.311      0.000       0.003       0.003\n",
      "NUMBER_OF_BORROWERS              -0.0652      0.002    -27.197      0.000      -0.070      -0.061\n",
      "POSTAL_CODE                   -4.157e-07   3.92e-08    -10.595      0.000   -4.93e-07   -3.39e-07\n",
      "FIRST_TIME_HOMEBUYER_FLAG_Y       0.0099      0.004      2.717      0.007       0.003       0.017\n",
      "LOAN_PURPOSE_N                   -0.0227      0.003     -6.494      0.000      -0.030      -0.016\n",
      "LOAN_PURPOSE_P                    0.1975      0.003     60.821      0.000       0.191       0.204\n",
      "==============================================================================\n",
      "Omnibus:                    35449.489   Durbin-Watson:                   1.999\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            53364.796\n",
      "Skew:                           1.024   Prob(JB):                         0.00\n",
      "Kurtosis:                       3.928   Cond. No.                     9.47e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 9.47e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Forward Selection (using Statsmodels for model selection)\n",
    "X_train_const = sm.add_constant(X_train)\n",
    "model = sm.OLS(y_train, X_train_const)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b02dfec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression MSE: 0.3324925884734913\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression:\n",
    "# Ridge regression adds a penalty term to control overfitting and reduce variance.\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "ridge_pred = ridge.predict(X_test)\n",
    "mse_ridge = mean_squared_error(y_test, ridge_pred)\n",
    "print(f'Ridge Regression MSE: {mse_ridge}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b68de4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha for Ridge Regression: 25.950242113997373\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation to find the best Î» value for Ridge Regression\n",
    "alphas = np.logspace(-4, 4, 100)\n",
    "ridge_cv = GridSearchCV(Ridge(), param_grid={'alpha': alphas}, scoring='neg_mean_squared_error', cv=5)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "best_alpha_ridge = ridge_cv.best_params_['alpha']\n",
    "print(f'Best alpha for Ridge Regression: {best_alpha_ridge}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "abb1b057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Ridge Regression MSE: 0.3324922462843652\n"
     ]
    }
   ],
   "source": [
    "# Refit with the best alpha\n",
    "ridge_best = Ridge(alpha=best_alpha_ridge)\n",
    "ridge_best.fit(X_train, y_train)\n",
    "ridge_best_pred = ridge_best.predict(X_test)\n",
    "mse_ridge_best = mean_squared_error(y_test, ridge_best_pred)\n",
    "print(f'Best Ridge Regression MSE: {mse_ridge_best}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "de9a1381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression MSE: 0.34460788801877\n"
     ]
    }
   ],
   "source": [
    "# Lasso Regression:\n",
    "# Lasso regression adds a penalty term to shrink unnecessary coefficients to zero, thus performing feature selection.\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train, y_train)\n",
    "lasso_pred = lasso.predict(X_test)\n",
    "mse_lasso = mean_squared_error(y_test, lasso_pred)\n",
    "print(f'Lasso Regression MSE: {mse_lasso}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e07b594e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha for Lasso Regression: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation to find the best Î» value for Lasso Regression\n",
    "lasso_cv = GridSearchCV(Lasso(), param_grid={'alpha': alphas}, scoring='neg_mean_squared_error', cv=5)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "best_alpha_lasso = lasso_cv.best_params_['alpha']\n",
    "print(f'Best alpha for Lasso Regression: {best_alpha_lasso}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bac7a9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Lasso Regression MSE: 0.33249533441281265\n"
     ]
    }
   ],
   "source": [
    "# Refit with the best alpha for Lasso\n",
    "lasso_best = Lasso(alpha=best_alpha_lasso)\n",
    "lasso_best.fit(X_train, y_train)\n",
    "lasso_best_pred = lasso_best.predict(X_test)\n",
    "mse_lasso_best = mean_squared_error(y_test, lasso_best_pred)\n",
    "print(f'Best Lasso Regression MSE: {mse_lasso_best}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3587b79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE: 0.33249260231929145\n",
      "Best Ridge Regression MSE: 0.3324922462843652\n",
      "Best Lasso Regression MSE: 0.33249533441281265\n"
     ]
    }
   ],
   "source": [
    "# Comparing the results of different models\n",
    "print(f'Linear Regression MSE: {mse_linear}')\n",
    "print(f'Best Ridge Regression MSE: {mse_ridge_best}')\n",
    "print(f'Best Lasso Regression MSE: {mse_lasso_best}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede28bc0",
   "metadata": {},
   "source": [
    "The MSE of our models are all rather similar, however you can see that Ridge regression out performs Lasso which out performs forward selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f67339",
   "metadata": {},
   "source": [
    "\n",
    "# Ethics\n",
    "## What are protected characteristics?\n",
    "https://www.youtube.com/watch?v=VXLtKlmtrvM&t=170s \n",
    "\n",
    "## Are they really protected:\n",
    "Maybe? Many people are trying to protect them. It can hard to prove discrimination. Humans are biased and therefore the models and programs they make and the data used to train models are biased.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5aff3d",
   "metadata": {},
   "source": [
    "### Examples in Data Science\n",
    "1. In October 2019, researchers found that an algorithm used on more than 200 million people in US hospitals to predict which patients would likely need extra medical care heavily favored white patients over black patients. While race itself wasnâ€™t a variable used in this algorithm, another variable highly correlated to race was, which was healthcare cost history. The rationale was that cost summarizes how many healthcare needs a particular person has. For various reasons, black patients incurred lower health-care costs than white patients with the same conditions on average. Thankfully, researchers worked with Optum to reduce the level of bias by 80%. But had they not interrogated in the first place, AI bias would have continued to discriminate severely.\n",
    "\n",
    "2. Arguably the most notable example of AI bias is the COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) algorithm used in US court systems to predict the likelihood that a defendant would become a recidivist. Due to the data that was used, the model that was chosen, and the process of creating the algorithm overall, the model predicted twice as many false positives for recidivism for black offenders (45%) than white offenders (23%).\n",
    "\n",
    "3. Amazonâ€™s one of the largest tech giants in the world. And so, itâ€™s no surprise that theyâ€™re heavy users of machine learning and artificial intelligence. In 2015, Amazon realized that their algorithm used for hiring employees was found to be biased against women. The reason for that was because the algorithm was based on the number of resumes submitted over the past ten years, and since most of the applicants were men, it was trained to favor men over women.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ad8eb0",
   "metadata": {},
   "source": [
    "### Ways Forward:\n",
    "1. Try to have models reflect how decisions should be made, not how they have been made historically. We don't want to perpetuate past biases.\n",
    "\n",
    "2. Check prediction accuracy (false negatives, false positive rate, etc.) across social groups to catch models that might be discriminatory.\n",
    "\n",
    "3. Consider ways of enforcing data governance and fairness. \n",
    "\n",
    "Source/further reading: https://towardsdatascience.com/real-life-examples-of-discriminating-artificial-intelligence-cae395a90070 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b21ddf",
   "metadata": {},
   "source": [
    "### Another Example\n",
    "related article: https://arxiv.org/pdf/2110.14755v1.pdf\n",
    "\n",
    "Discusses Machine Learning identifying race as a short cut when trying to gleam medical information off of a chest x-ray. This contrasts with human identifiers who are unable to (or at least not trained to) look for racial identifiers when looking at chest x-rays. This is an example of how algorithms, specifically black box algorithms, could potentially be using protected characteristics in decisions even when these protected characteristics are not directly identified in the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2c67340a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREDIT_SCORE                     float64\n",
      "ORIGINAL_DEBT_TO_INCOME_RATIO    float64\n",
      "NUMBER_OF_BORROWERS              float64\n",
      "POSTAL_CODE                      float64\n",
      "FIRST_TIME_HOMEBUYER_FLAG_Y         bool\n",
      "LOAN_PURPOSE_N                      bool\n",
      "LOAN_PURPOSE_P                      bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775a9427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
